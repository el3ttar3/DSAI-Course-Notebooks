{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 201660,
     "status": "ok",
     "timestamp": 1761739463491,
     "user": {
      "displayName": "Abdulrahman Mohammed 202201353",
      "userId": "07518075019330847785"
     },
     "user_tz": -180
    },
    "id": "rFILUrMZ1iIJ",
    "outputId": "d6725cac-513d-4199-b5b1-ce5370221738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m813.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m144.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m141.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Train shape: (54000, 784), Val shape: (6000, 784), Test shape: (10000, 784)\n",
      "Running PSO optimization (this will train multiple small models)...\n",
      "[Init] Particle 00 pos=[3.80794718e-03 4.77821438e-01 9.79833215e+01] val_loss=0.392332\n",
      "[Init] Particle 01 pos=[6.02671899e-03 1.20208388e-01 3.34713863e+01] val_loss=0.429988\n",
      "[Init] Particle 02 pos=[6.75027760e-04 4.39779266e-01 8.33248813e+01] val_loss=0.376252\n",
      "[Init] Particle 03 pos=[7.10991852e-03 5.92630224e-02 1.24629903e+02] val_loss=0.383587\n",
      "\n",
      "Initial global best idx=2, val_loss=0.376252, params=[6.75027760e-04 4.39779266e-01 8.33248813e+01]\n",
      "\n",
      "=== Iteration 1/5 ===\n",
      "Particle 00 | pos=[0.00283722, 0.454125, 77.9497] | val_loss=0.385057 | pbest_val=0.385057 | improved=True\n",
      "Particle 01 | pos=[0.00205458, 0.166375, 70.889] | val_loss=0.352060 | pbest_val=0.352060 | improved=True\n",
      "Particle 02 | pos=[0.000580704, 0.426627, 85.0787] | val_loss=0.370592 | pbest_val=0.370592 | improved=True\n",
      "Particle 03 | pos=[0.000215369, 0.224085, 90.3123] | val_loss=0.390454 | pbest_val=0.383587 | improved=False\n",
      "Iteration 1 summary: gbest_val=0.352060 params=[0.00205458, 0.166375, 70.889] improved=True\n",
      "Iteration time: 23.8s\n",
      "\n",
      "=== Iteration 2/5 ===\n",
      "Particle 00 | pos=[0.00170141, 0.320417, 55.149] | val_loss=0.362541 | pbest_val=0.362541 | improved=True\n",
      "Particle 01 | pos=[0.0001, 0.198692, 97.0814] | val_loss=0.417440 | pbest_val=0.352060 | improved=False\n",
      "Particle 02 | pos=[0.000826233, 0.104261, 84.7197] | val_loss=0.350632 | pbest_val=0.350632 | improved=True\n",
      "Particle 03 | pos=[0.000138641, 0.26143, 77.2473] | val_loss=0.406470 | pbest_val=0.383587 | improved=False\n",
      "Iteration 2 summary: gbest_val=0.350632 params=[0.000826233, 0.104261, 84.7197] improved=True\n",
      "Iteration time: 25.3s\n",
      "\n",
      "=== Iteration 3/5 ===\n",
      "Particle 00 | pos=[0.000498104, 0.121386, 71.5509] | val_loss=0.339653 | pbest_val=0.339653 | improved=True\n",
      "Particle 01 | pos=[0.000985525, 0.0582552, 103.751] | val_loss=0.334943 | pbest_val=0.334943 | improved=True\n",
      "Particle 02 | pos=[0.000998104, 0.05, 84.4683] | val_loss=0.345346 | pbest_val=0.345346 | improved=True\n",
      "Particle 03 | pos=[0.0071816, 0.05, 78.1538] | val_loss=0.390206 | pbest_val=0.383587 | improved=False\n",
      "Iteration 3 summary: gbest_val=0.334943 params=[0.000985525, 0.0582552, 103.751] improved=True\n",
      "Iteration time: 23.9s\n",
      "\n",
      "=== Iteration 4/5 ===\n",
      "Particle 00 | pos=[0.0001, 0.05, 97.0272] | val_loss=0.425536 | pbest_val=0.339653 | improved=False\n",
      "Particle 01 | pos=[0.00160539, 0.05, 108.419] | val_loss=0.365131 | pbest_val=0.334943 | improved=False\n",
      "Particle 02 | pos=[0.00110646, 0.05, 107.538] | val_loss=0.351954 | pbest_val=0.345346 | improved=False\n",
      "Particle 03 | pos=[0.01, 0.05, 128] | val_loss=0.401992 | pbest_val=0.383587 | improved=False\n",
      "Iteration 4 summary: gbest_val=0.334943 params=[0.000985525, 0.0582552, 103.751] improved=False\n",
      "Iteration time: 21.4s\n",
      "\n",
      "=== Iteration 5/5 ===\n",
      "Particle 00 | pos=[0.000268858, 0.05, 112.218] | val_loss=0.397381 | pbest_val=0.339653 | improved=False\n",
      "Particle 01 | pos=[0.00163646, 0.05, 103.773] | val_loss=0.354051 | pbest_val=0.334943 | improved=False\n",
      "Particle 02 | pos=[0.000983391, 0.05, 117.979] | val_loss=0.352306 | pbest_val=0.345346 | improved=False\n",
      "Particle 03 | pos=[0.00659981, 0.05, 128] | val_loss=0.390762 | pbest_val=0.383587 | improved=False\n",
      "Iteration 5 summary: gbest_val=0.334943 params=[0.000985525, 0.0582552, 103.751] improved=False\n",
      "Iteration time: 21.6s\n",
      "\n",
      "PSO finished in 116.0s. Best val_loss=0.334943, best_params=[9.85525112e-04 5.82551721e-02 1.03750820e+02]\n",
      "\n",
      "=== PSO Result ===\n",
      "Best learning rate : 0.00098553\n",
      "Best dropout       : 0.058255\n",
      "Best batch size    : 104\n",
      "Best validation loss: 0.334943\n"
     ]
    }
   ],
   "source": [
    "# PSO FROM SCRATCH — Hyperparameter tuning for a Keras model (minimize validation loss)\n",
    "# Paste this entire block into a Colab cell and run.\n",
    "!pip install tensorflow numpy matplotlib pandas scikit-learn -q\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# -------------------------\n",
    "# Configuration / Settings\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# PSO hyperparameters\n",
    "NUM_PARTICLES = 4        # reduce for faster run during tests\n",
    "MAX_ITERS = 5            # number of PSO iterations (reduce for quick tests)\n",
    "INERTIA = 0.7            # w\n",
    "C1 = 1.5                 # cognitive coeff\n",
    "C2 = 1.5                 # social coeff\n",
    "VELOCITY_CLAMP = None    # set to a tuple (min,max) per-dimension if desired\n",
    "\n",
    "# Objective (model training) hyperparameters\n",
    "EPOCHS_PER_EVAL = 3      # epochs used during each particle evaluation (keep small for speed)\n",
    "VERBOSE = True           # whether to print detailed per iteration info\n",
    "\n",
    "# Search bounds for hyperparameters: [(lr_min, lr_max), (drop_min, drop_max), (batch_min, batch_max)]\n",
    "BOUNDS = [(1e-4, 1e-2),   # learning rate\n",
    "          (0.05, 0.5),    # dropout rate\n",
    "          (16, 128)]      # batch size (treated as integer)\n",
    "\n",
    "# -------------------------\n",
    "# Load dataset & split\n",
    "# -------------------------\n",
    "# We'll use Fashion MNIST for demonstration\n",
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train_full = x_train_full.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "# Flatten for simple dense network\n",
    "x_train_full = x_train_full.reshape((-1, 28*28))\n",
    "x_test = x_test.reshape((-1, 28*28))\n",
    "\n",
    "# Create a fixed validation split for consistent val_loss reporting across particles\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_full, y_train_full, test_size=0.1, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {x_train.shape}, Val shape: {x_val.shape}, Test shape: {x_test.shape}\")\n",
    "\n",
    "# -------------------------\n",
    "# Model factory\n",
    "# -------------------------\n",
    "def create_model(learning_rate, dropout_rate):\n",
    "    \"\"\"\n",
    "    Create a simple feed-forward model. We clear session before creating to avoid TF memory accumulation.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(784,)),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dropout(dropout_rate),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Objective function\n",
    "# -------------------------\n",
    "def evaluate_hyperparams(params, epochs=EPOCHS_PER_EVAL, verbose_fit=0):\n",
    "    \"\"\"\n",
    "    params: array-like [lr, dropout, batch]\n",
    "    Returns: validation loss (float)\n",
    "    \"\"\"\n",
    "    lr = float(params[0])\n",
    "    dropout = float(params[1])\n",
    "    batch = int(np.round(params[2]))\n",
    "    batch = max(1, batch)  # ensure positive\n",
    "\n",
    "    # Build and train the model (small epochs to keep runtime reasonable)\n",
    "    model = create_model(lr, dropout)\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch,\n",
    "        verbose=verbose_fit\n",
    "    )\n",
    "    # We use the last epoch validation loss as the fitness\n",
    "    val_loss = float(history.history['val_loss'][-1])\n",
    "    # To help free memory, delete model (session clearing done at next model creation)\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    return val_loss\n",
    "\n",
    "# -------------------------\n",
    "# PSO Implementation\n",
    "# -------------------------\n",
    "def pso_optimize(func, bounds, num_particles=NUM_PARTICLES, max_iter=MAX_ITERS,\n",
    "                 w=INERTIA, c1=C1, c2=C2, velocity_clamp=VELOCITY_CLAMP, verbose=VERBOSE):\n",
    "    \"\"\"\n",
    "    func: function that takes a vector x and returns scalar fitness (lower is better)\n",
    "    bounds: list of (low, high) per dimension\n",
    "    \"\"\"\n",
    "    dim = len(bounds)\n",
    "    lb = np.array([b[0] for b in bounds], dtype=float)\n",
    "    ub = np.array([b[1] for b in bounds], dtype=float)\n",
    "\n",
    "    # Initialize particle positions and velocities\n",
    "    X = np.random.uniform(lb, ub, (num_particles, dim))\n",
    "    # Initialize small random velocities\n",
    "    V = np.random.uniform(-0.1*(ub-lb), 0.1*(ub-lb), (num_particles, dim))\n",
    "\n",
    "    # Personal bests\n",
    "    pbest = X.copy()\n",
    "    pbest_val = np.array([np.inf]*num_particles, dtype=float)\n",
    "\n",
    "    # Evaluate initial particles\n",
    "    for i in range(num_particles):\n",
    "        try:\n",
    "            fitness = func(X[i])\n",
    "        except Exception as e:\n",
    "            fitness = np.inf\n",
    "            print(\"Error evaluating particle\", i, e)\n",
    "        pbest_val[i] = fitness\n",
    "        if verbose:\n",
    "            print(f\"[Init] Particle {i:02d} pos={X[i]} val_loss={fitness:.6f}\")\n",
    "\n",
    "    # Global best\n",
    "    gbest_idx = int(np.argmin(pbest_val))\n",
    "    gbest = pbest[gbest_idx].copy()\n",
    "    gbest_val = float(pbest_val[gbest_idx])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nInitial global best idx={gbest_idx}, val_loss={gbest_val:.6f}, params={gbest}\\n\")\n",
    "\n",
    "    # PSO main loop\n",
    "    history_gbest = [gbest_val]\n",
    "    start_time = time.time()\n",
    "    for t in range(max_iter):\n",
    "        iter_start = time.time()\n",
    "        if verbose:\n",
    "            print(f\"=== Iteration {t+1}/{max_iter} ===\")\n",
    "\n",
    "        # Update velocity and position for all particles\n",
    "        r1 = np.random.rand(num_particles, dim)\n",
    "        r2 = np.random.rand(num_particles, dim)\n",
    "\n",
    "        V = (w * V\n",
    "             + c1 * r1 * (pbest - X)\n",
    "             + c2 * r2 * (gbest - X))\n",
    "\n",
    "        # Optionally clamp velocities\n",
    "        if velocity_clamp is not None:\n",
    "            V = np.clip(V, velocity_clamp[0], velocity_clamp[1])\n",
    "\n",
    "        # Update positions and clip to bounds\n",
    "        X = X + V\n",
    "        X = np.clip(X, lb, ub)\n",
    "\n",
    "        # Evaluate each particle and update pbest\n",
    "        for i in range(num_particles):\n",
    "            val = func(X[i])\n",
    "            if val < pbest_val[i]:\n",
    "                pbest_val[i] = val\n",
    "                pbest[i] = X[i].copy()\n",
    "                improved = True\n",
    "            else:\n",
    "                improved = False\n",
    "\n",
    "            if verbose:\n",
    "                pos_pretty = \", \".join([f\"{v:.6g}\" for v in X[i]])\n",
    "                pbest_pretty = \", \".join([f\"{v:.6g}\" for v in pbest[i]])\n",
    "                print(f\"Particle {i:02d} | pos=[{pos_pretty}] | val_loss={val:.6f} | pbest_val={pbest_val[i]:.6f} | improved={improved}\")\n",
    "\n",
    "        # Update global best\n",
    "        min_idx = int(np.argmin(pbest_val))\n",
    "        if pbest_val[min_idx] < gbest_val:\n",
    "            gbest_val = float(pbest_val[min_idx])\n",
    "            gbest = pbest[min_idx].copy()\n",
    "            g_improved = True\n",
    "        else:\n",
    "            g_improved = False\n",
    "\n",
    "        history_gbest.append(gbest_val)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Iteration {t+1} summary: gbest_val={gbest_val:.6f} params=[{', '.join(f'{v:.6g}' for v in gbest)}] improved={g_improved}\")\n",
    "            print(f\"Iteration time: {time.time()-iter_start:.1f}s\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    if verbose:\n",
    "        print(f\"PSO finished in {total_time:.1f}s. Best val_loss={gbest_val:.6f}, best_params={gbest}\")\n",
    "\n",
    "    return gbest, gbest_val, history_gbest\n",
    "\n",
    "# -------------------------\n",
    "# Run PSO\n",
    "# -------------------------\n",
    "print(\"Running PSO optimization (this will train multiple small models)...\")\n",
    "best_params, best_val, g_history = pso_optimize(\n",
    "    evaluate_hyperparams,\n",
    "    bounds=BOUNDS,\n",
    "    num_particles=NUM_PARTICLES,\n",
    "    max_iter=MAX_ITERS,\n",
    "    w=INERTIA, c1=C1, c2=C2,\n",
    "    velocity_clamp=VELOCITY_CLAMP,\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "# Post-process best params (batch -> int)\n",
    "best_lr = float(best_params[0])\n",
    "best_dropout = float(best_params[1])\n",
    "best_batch = int(np.round(best_params[2]))\n",
    "best_batch = max(1, best_batch)\n",
    "\n",
    "print(\"\\n=== PSO Result ===\")\n",
    "print(f\"Best learning rate : {best_lr:.8f}\")\n",
    "print(f\"Best dropout       : {best_dropout:.6f}\")\n",
    "print(f\"Best batch size    : {best_batch}\")\n",
    "print(f\"Best validation loss: {best_val:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12727,
     "status": "ok",
     "timestamp": 1761739495718,
     "user": {
      "displayName": "Abdulrahman Mohammed 202201353",
      "userId": "07518075019330847785"
     },
     "user_tz": -180
    },
    "id": "2PkeQCW-1mg7",
    "outputId": "2a1717ea-22ab-40ff-92e8-736e2582e6fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with PSO-tuned hyperparameters for 8 epochs...\n",
      "Epoch 1/8\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.6917 - val_accuracy: 0.8548 - val_loss: 0.4029\n",
      "Epoch 2/8\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8600 - loss: 0.3903 - val_accuracy: 0.8700 - val_loss: 0.3717\n",
      "Epoch 3/8\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8736 - loss: 0.3456 - val_accuracy: 0.8750 - val_loss: 0.3461\n",
      "Epoch 4/8\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8814 - loss: 0.3215 - val_accuracy: 0.8787 - val_loss: 0.3399\n",
      "Epoch 5/8\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8893 - loss: 0.3000 - val_accuracy: 0.8735 - val_loss: 0.3463\n",
      "Epoch 6/8\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8942 - loss: 0.2827 - val_accuracy: 0.8770 - val_loss: 0.3531\n",
      "Epoch 7/8\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8991 - loss: 0.2691 - val_accuracy: 0.8800 - val_loss: 0.3545\n",
      "Epoch 8/8\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9048 - loss: 0.2584 - val_accuracy: 0.8785 - val_loss: 0.3519\n",
      "Final test accuracy: 0.8714, test loss: 0.371197\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Train final model with best params (longer training if desired)\n",
    "# -------------------------\n",
    "final_epochs = 8\n",
    "print(f\"\\nTraining final model with PSO-tuned hyperparameters for {final_epochs} epochs...\")\n",
    "final_model = create_model(best_lr, best_dropout)\n",
    "final_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=final_epochs,\n",
    "                batch_size=best_batch, verbose=1)\n",
    "final_test_loss, final_test_acc = final_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Final test accuracy: {final_test_acc:.4f}, test loss: {final_test_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOp89xRgVKm4zZvcIGAP+01",
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
