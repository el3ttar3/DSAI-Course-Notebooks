{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Lab 3: Simulated Annealing for House Price Prediction\n\n## Task\nLoad the \"house_prices\" dataset from OpenML, apply simulated annealing, and evaluate the results.\n\n### Learning Objectives\n- Understand and implement the Simulated Annealing optimization algorithm\n- Apply metaheuristic optimization to a real-world regression problem\n- Evaluate the effectiveness of nature-inspired algorithms for parameter optimization"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 1. Setup and Dependencies"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport math\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(42)\nrandom.seed(42)\n\nprint(\"All libraries imported successfully!\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 2. Load Data\n\nWe use `sklearn.datasets.fetch_openml` which is more stable than the direct openml library (avoids minio dependency conflicts)."]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print(\"Loading house_prices dataset from OpenML...\")\nhouse_prices = fetch_openml(name='house_prices', version=1, as_frame=True, parser='auto')\nhouse_data = house_prices.frame\n\nprint(f\"Dataset loaded! Shape: {house_data.shape}\")\nprint(f\"\\nTarget statistics:\")\nprint(house_data['SalePrice'].describe())\nhouse_data.head()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 3. Data Preprocessing\n\nHandle missing values, encode categorical features, and scale numerical features."]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["X = house_data.drop('SalePrice', axis=1)\ny = house_data['SalePrice'].astype(float)\n\nnumerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(f\"Numerical features: {len(numerical_features)}\")\nprint(f\"Categorical features: {len(categorical_features)}\")\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_features),\n    ('cat', categorical_transformer, categorical_features)\n])\n\nX_preprocessed = preprocessor.fit_transform(X)\nprint(f\"Preprocessed data shape: {X_preprocessed.shape}\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["X_train, X_test, y_train, y_test = train_test_split(\n    X_preprocessed, y, test_size=0.2, random_state=42\n)\nprint(f\"Training: {X_train.shape[0]} samples, Test: {X_test.shape[0]} samples\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 4. Objective Function\n\nThe objective function calculates Mean Squared Error (MSE) for given linear model coefficients."]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["def objective_function(params, X, y):\n    intercept = params[0]\n    coefficients = params[1:]\n    y_pred = X @ coefficients + intercept\n    return np.mean((y - y_pred) ** 2)\n\nn_features = X_train.shape[1]\ninitial_mse = objective_function(np.zeros(n_features + 1), X_train, y_train.values)\nprint(f\"Initial MSE with zero parameters: {initial_mse:,.2f}\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 5. Simulated Annealing Implementation\n\nSimulated Annealing is a probabilistic optimization technique inspired by metallurgy annealing:\n1. Start with initial solution and high temperature\n2. Generate neighbor solutions with small random perturbations\n3. Accept better solutions always; accept worse solutions with probability based on temperature\n4. Gradually cool down temperature\n5. Repeat until convergence"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["def simulated_annealing(objective_func, X, y, n_params, \n                        initial_temp=1000.0, cooling_rate=0.995,\n                        num_iterations=5000, step_size=0.5):\n    current_solution = np.random.randn(n_params) * 0.01\n    best_solution = current_solution.copy()\n    current_cost = objective_func(current_solution, X, y)\n    best_cost = current_cost\n    temperature = initial_temp\n    history = [current_cost]\n    \n    print(\"Starting Simulated Annealing...\")\n    print(\"=\"*60)\n    \n    for i in range(num_iterations):\n        neighbor_solution = current_solution + np.random.randn(n_params) * step_size\n        neighbor_cost = objective_func(neighbor_solution, X, y)\n        cost_diff = neighbor_cost - current_cost\n        \n        if cost_diff < 0:\n            acceptance_prob = 1.0\n        else:\n            acceptance_prob = math.exp(-cost_diff / (temperature + 1e-10))\n        \n        if random.random() < acceptance_prob:\n            current_solution = neighbor_solution\n            current_cost = neighbor_cost\n        \n        if current_cost < best_cost:\n            best_solution = current_solution.copy()\n            best_cost = current_cost\n        \n        temperature *= cooling_rate\n        history.append(best_cost)\n        \n        if (i + 1) % 1000 == 0:\n            print(f\"Iter {i+1:5d} | Best MSE: {best_cost:,.0f} | Temp: {temperature:.4f}\")\n    \n    print(\"=\"*60)\n    return best_solution, best_cost, history"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["n_params = X_train.shape[1] + 1\n\nbest_params, best_mse, cost_history = simulated_annealing(\n    objective_function, X_train, y_train.values, n_params,\n    initial_temp=1000.0, cooling_rate=0.995, num_iterations=5000, step_size=0.5\n)\n\nprint(f\"\\nBest Training MSE: {best_mse:,.2f}\")\nprint(f\"Best Training RMSE: ${np.sqrt(best_mse):,.2f}\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 6. Visualization"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["plt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(cost_history, 'b-', alpha=0.7)\nplt.xlabel('Iteration')\nplt.ylabel('MSE')\nplt.title('Simulated Annealing Convergence')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(cost_history, 'b-', alpha=0.7)\nplt.xlabel('Iteration')\nplt.ylabel('MSE (Log Scale)')\nplt.title('Convergence (Log Scale)')\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nimprovement = (cost_history[0] - cost_history[-1]) / cost_history[0] * 100\nprint(f\"Improvement: {improvement:.1f}%\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 7. Evaluation and Comparison"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Evaluate on test set\ntest_mse = objective_function(best_params, X_test, y_test.values)\ntest_rmse = np.sqrt(test_mse)\n\n# Compare with sklearn LinearRegression\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\ny_pred_sklearn = lr_model.predict(X_test)\nsklearn_mse = mean_squared_error(y_test, y_pred_sklearn)\nsklearn_rmse = np.sqrt(sklearn_mse)\n\nprint(\"Results Comparison:\")\nprint(\"=\"*55)\nprint(f\"{'Method':<25} {'Test MSE':>14} {'Test RMSE':>14}\")\nprint(\"-\"*55)\nprint(f\"{'Simulated Annealing':<25} {test_mse:>14,.0f} {test_rmse:>14,.2f}\")\nprint(f\"{'sklearn LinearRegression':<25} {sklearn_mse:>14,.0f} {sklearn_rmse:>14,.2f}\")\nprint(\"=\"*55)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Plot predictions\ny_pred_sa = X_test @ best_params[1:] + best_params[0]\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred_sa, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.title('Simulated Annealing')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, y_pred_sklearn, alpha=0.5, color='orange')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.title('sklearn Linear Regression')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 8. Summary\n\n### Key Findings\n- Successfully implemented Simulated Annealing for optimizing linear regression coefficients\n- The algorithm demonstrated convergence by reducing MSE over iterations\n- Compared results with sklearn analytical solution (OLS)\n\n### Observations\n- SA is a metaheuristic that may not reach the exact optimal but can approach it\n- The strength of SA lies in its ability to escape local minima and handle complex optimization landscapes\n- For linear regression specifically, analytical methods like OLS are more efficient\n\n### Potential Improvements\n- Tune hyperparameters (temperature, cooling rate, iterations)\n- Use more complex models beyond linear regression\n- Apply feature selection to reduce dimensionality"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.10.0"},
    "colab": {"provenance": []}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}