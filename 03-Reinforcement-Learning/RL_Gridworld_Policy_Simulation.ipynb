{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1089978,
     "status": "ok",
     "timestamp": 1760267556913,
     "user": {
      "displayName": "Abdulrahman Mohammed 202201353",
      "userId": "07518075019330847785"
     },
     "user_tz": -180
    },
    "id": "BvgAQ4gIBC4i",
    "outputId": "4b819da8-6242-4210-8c92-f4dd846aa100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ Running simulation for Reward Setting 1...\n",
      "\n",
      "⏳ Running simulation for Reward Setting 2...\n",
      "\n",
      "---\n",
      "\n",
      "## Value Function under Reward Setting 1\n",
      "Rewards: Off-grid=-1, A=+10, B=+5, Other=0\n",
      "[[ 5.3  8.4  4.9  4.6  3. ]\n",
      " [ 2.6  3.1  2.2  1.5  1.3]\n",
      " [ 0.7  0.7  0.4 -0.2  0.2]\n",
      " [-0.2 -0.4 -0.3 -0.3 -0.3]\n",
      " [-0.5 -0.9 -0.6 -0.5 -0.5]]\n",
      "\n",
      "---\n",
      "\n",
      "## Value Function under Reward Setting 2\n",
      "Rewards: Off-grid=5, A=+16, B=+11, Other=6\n",
      "[[62.2 62.8 61.7 58.8 59.4]\n",
      " [55.7 54.2 52.6 51.7 53.6]\n",
      " [50.1 47.  45.2 43.2 48.4]\n",
      " [46.3 42.  41.7 42.4 47. ]\n",
      " [47.  40.7 43.1 45.2 49.1]]\n",
      "\n",
      "---\n",
      "\n",
      "## Difference (V_setting2 - V_setting1)\n",
      "[[57.  54.4 56.8 54.2 56.4]\n",
      " [53.1 51.1 50.5 50.2 52.3]\n",
      " [49.4 46.3 44.8 43.4 48.2]\n",
      " [46.5 42.4 42.  42.7 47.3]\n",
      " [47.5 41.6 43.8 45.7 49.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# --- Gridworld Environment Configuration ---\n",
    "GRID_SIZE = 5\n",
    "A_POS = (0, 1)\n",
    "A_PRIME_POS = (4, 1)\n",
    "B_POS = (0, 3)\n",
    "B_PRIME_POS = (2, 3)\n",
    "GAMMA = 0.9  # Discount factor\n",
    "\n",
    "# --- Simulation Configuration ---\n",
    "NUM_EPISODES = 50000\n",
    "MAX_STEPS_PER_EPISODE = 1000\n",
    "\n",
    "# Actions: 0: North, 1: South, 2: East, 3: West\n",
    "ACTIONS = [np.array([-1, 0]), np.array([1, 0]), np.array([0, 1]), np.array([0, -1])]\n",
    "\n",
    "def get_policy(state):\n",
    "    \"\"\"\n",
    "    Defines the policy pi(a|s).\n",
    "    For this problem, we use a uniform random policy.\n",
    "    \"\"\"\n",
    "    return np.random.choice(len(ACTIONS))\n",
    "\n",
    "def get_next_state_and_reward(state, action, reward_setting=1):\n",
    "    \"\"\"\n",
    "    Returns the next state and reward based on the current state and action.\n",
    "    \"\"\"\n",
    "    # Handle special states A and B\n",
    "    if state == A_POS:\n",
    "        reward = 10 if reward_setting == 1 else 16\n",
    "        return A_PRIME_POS, reward\n",
    "    if state == B_POS:\n",
    "        reward = 5 if reward_setting == 1 else 11\n",
    "        return B_PRIME_POS, reward\n",
    "\n",
    "    # Calculate potential next state\n",
    "    next_state = tuple((np.array(state) + ACTIONS[action]))\n",
    "\n",
    "    # Check for moving off the grid\n",
    "    if not (0 <= next_state[0] < GRID_SIZE and 0 <= next_state[1] < GRID_SIZE):\n",
    "        reward = -1 if reward_setting == 1 else 5\n",
    "        return state, reward # Stay in the same state\n",
    "\n",
    "    # Standard move\n",
    "    reward = 0 if reward_setting == 1 else 6\n",
    "    return next_state, reward\n",
    "\n",
    "def compute_value_function(reward_setting):\n",
    "    \"\"\"\n",
    "    Performs Monte Carlo simulation to compute the state-value function.\n",
    "    \"\"\"\n",
    "    # Initialize returns dictionary to store a list of returns for each state\n",
    "    returns = collections.defaultdict(list)\n",
    "\n",
    "    print(f\"\\n⏳ Running simulation for Reward Setting {reward_setting}...\")\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        # Start each episode from a random state\n",
    "        initial_state = (np.random.randint(GRID_SIZE), np.random.randint(GRID_SIZE))\n",
    "        current_state = initial_state\n",
    "        episode_history = []\n",
    "\n",
    "        # Generate an episode\n",
    "        for _ in range(MAX_STEPS_PER_EPISODE):\n",
    "            action = get_policy(current_state)\n",
    "            next_state, reward = get_next_state_and_reward(current_state, action, reward_setting)\n",
    "            episode_history.append((current_state, reward))\n",
    "            current_state = next_state\n",
    "\n",
    "        # Calculate returns for the episode (First-Visit MC)\n",
    "        G = 0\n",
    "        visited_states = set()\n",
    "        # Iterate backwards through the episode history\n",
    "        for state, reward in reversed(episode_history):\n",
    "            G = reward + GAMMA * G\n",
    "            # If this is the first time we've visited the state in this episode\n",
    "            if state not in visited_states:\n",
    "                returns[state].append(G)\n",
    "                visited_states.add(state)\n",
    "\n",
    "    # Calculate the average return for each state to get the value function\n",
    "    value_function = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    for r in range(GRID_SIZE):\n",
    "        for c in range(GRID_SIZE):\n",
    "            state = (r, c)\n",
    "            if returns[state]:\n",
    "                value_function[r, c] = np.mean(returns[state])\n",
    "\n",
    "    return value_function\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Calculate value function for the first reward setting\n",
    "v_setting1 = compute_value_function(reward_setting=1)\n",
    "\n",
    "# Calculate value function for the second reward setting\n",
    "v_setting2 = compute_value_function(reward_setting=2)\n",
    "\n",
    "# --- Display Results ---\n",
    "np.set_printoptions(precision=1, suppress=True)\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"\\n## Value Function under Reward Setting 1\")\n",
    "print(\"Rewards: Off-grid=-1, A=+10, B=+5, Other=0\")\n",
    "print(v_setting1)\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"\\n## Value Function under Reward Setting 2\")\n",
    "print(\"Rewards: Off-grid=5, A=+16, B=+11, Other=6\")\n",
    "print(v_setting2)\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"\\n## Difference (V_setting2 - V_setting1)\")\n",
    "print(v_setting2 - v_setting1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0+Me6a0W9gO/ThNCBxJ1H",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
